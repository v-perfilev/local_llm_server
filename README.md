# Local LLM Server

A Flask server deploying the Llama-2 chat model with optional quantization.

